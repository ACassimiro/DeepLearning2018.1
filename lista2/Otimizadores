Quanto aos otimizadores, eu usei o otimizador descrito em http://ruder.io/optimizing-gradient-descent/, por funcionar bem em uma série de superfícies distintas, ao utilizar learning rates distintos para cada parâmetro, além de manter uma taxa de decaimento exponencial do quadrado dos antigos gradientes como o RMSprop ou ou Adadelta, ele também mantem uma taxa de decaimento dos antigos gradientes em si, simulando uma espécie de momento. Para constrastar com esse otimizador bastante complexo, eu decidi usar como segundo otimizador o SGD com momento, sem decaimento nem gradiente de Nesterov, para ver a diferença em performance.
